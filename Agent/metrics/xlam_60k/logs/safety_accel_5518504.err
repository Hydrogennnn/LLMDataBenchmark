The following values were not passed to `accelerate launch` and had defaults used instead:
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   6%|▌         | 1/17 [00:00<00:02,  5.53it/s]Loading checkpoint shards:   6%|▌         | 1/17 [00:00<00:02,  5.65it/s]Loading checkpoint shards:   6%|▌         | 1/17 [00:00<00:02,  5.61it/s]Loading checkpoint shards:   6%|▌         | 1/17 [00:00<00:02,  5.59it/s]Loading checkpoint shards:   6%|▌         | 1/17 [00:00<00:02,  5.61it/s]Loading checkpoint shards:   6%|▌         | 1/17 [00:00<00:02,  5.96it/s]Loading checkpoint shards:   6%|▌         | 1/17 [00:00<00:02,  5.85it/s]Loading checkpoint shards:   6%|▌         | 1/17 [00:00<00:02,  6.04it/s]Loading checkpoint shards:  12%|█▏        | 2/17 [00:00<00:02,  5.54it/s]Loading checkpoint shards:  12%|█▏        | 2/17 [00:00<00:02,  5.51it/s]Loading checkpoint shards:  12%|█▏        | 2/17 [00:00<00:02,  5.68it/s]Loading checkpoint shards:  12%|█▏        | 2/17 [00:00<00:02,  5.64it/s]Loading checkpoint shards:  12%|█▏        | 2/17 [00:00<00:02,  5.71it/s]Loading checkpoint shards:  12%|█▏        | 2/17 [00:00<00:02,  5.54it/s]Loading checkpoint shards:  12%|█▏        | 2/17 [00:00<00:02,  5.55it/s]Loading checkpoint shards:  12%|█▏        | 2/17 [00:00<00:02,  5.52it/s]Loading checkpoint shards:  18%|█▊        | 3/17 [00:00<00:02,  5.43it/s]Loading checkpoint shards:  18%|█▊        | 3/17 [00:00<00:02,  5.45it/s]Loading checkpoint shards:  18%|█▊        | 3/17 [00:00<00:02,  5.50it/s]Loading checkpoint shards:  18%|█▊        | 3/17 [00:00<00:02,  5.54it/s]Loading checkpoint shards:  18%|█▊        | 3/17 [00:00<00:02,  5.45it/s]Loading checkpoint shards:  18%|█▊        | 3/17 [00:00<00:02,  5.45it/s]Loading checkpoint shards:  18%|█▊        | 3/17 [00:00<00:02,  5.46it/s]Loading checkpoint shards:  18%|█▊        | 3/17 [00:00<00:02,  5.52it/s]Loading checkpoint shards:  24%|██▎       | 4/17 [00:00<00:02,  4.95it/s]Loading checkpoint shards:  24%|██▎       | 4/17 [00:00<00:02,  4.96it/s]Loading checkpoint shards:  24%|██▎       | 4/17 [00:00<00:02,  4.92it/s]Loading checkpoint shards:  24%|██▎       | 4/17 [00:00<00:02,  4.92it/s]Loading checkpoint shards:  24%|██▎       | 4/17 [00:00<00:02,  4.92it/s]Loading checkpoint shards:  24%|██▎       | 4/17 [00:00<00:02,  4.94it/s]Loading checkpoint shards:  24%|██▎       | 4/17 [00:00<00:02,  4.92it/s]Loading checkpoint shards:  24%|██▎       | 4/17 [00:00<00:02,  4.90it/s]Loading checkpoint shards:  29%|██▉       | 5/17 [00:01<00:02,  4.66it/s]Loading checkpoint shards:  29%|██▉       | 5/17 [00:01<00:02,  4.66it/s]Loading checkpoint shards:  29%|██▉       | 5/17 [00:01<00:02,  4.68it/s]Loading checkpoint shards:  29%|██▉       | 5/17 [00:01<00:02,  4.66it/s]Loading checkpoint shards:  29%|██▉       | 5/17 [00:01<00:02,  4.69it/s]Loading checkpoint shards:  29%|██▉       | 5/17 [00:01<00:02,  4.66it/s]Loading checkpoint shards:  29%|██▉       | 5/17 [00:01<00:02,  4.68it/s]Loading checkpoint shards:  29%|██▉       | 5/17 [00:01<00:02,  4.66it/s]Loading checkpoint shards:  35%|███▌      | 6/17 [00:01<00:02,  4.52it/s]Loading checkpoint shards:  35%|███▌      | 6/17 [00:01<00:02,  4.51it/s]Loading checkpoint shards:  35%|███▌      | 6/17 [00:01<00:02,  4.51it/s]Loading checkpoint shards:  35%|███▌      | 6/17 [00:01<00:02,  4.52it/s]Loading checkpoint shards:  35%|███▌      | 6/17 [00:01<00:02,  4.53it/s]Loading checkpoint shards:  35%|███▌      | 6/17 [00:01<00:02,  4.51it/s]Loading checkpoint shards:  35%|███▌      | 6/17 [00:01<00:02,  4.51it/s]Loading checkpoint shards:  35%|███▌      | 6/17 [00:01<00:02,  4.51it/s]Loading checkpoint shards:  41%|████      | 7/17 [00:01<00:02,  4.52it/s]Loading checkpoint shards:  41%|████      | 7/17 [00:01<00:02,  4.52it/s]Loading checkpoint shards:  41%|████      | 7/17 [00:01<00:02,  4.53it/s]Loading checkpoint shards:  41%|████      | 7/17 [00:01<00:02,  4.52it/s]Loading checkpoint shards:  41%|████      | 7/17 [00:01<00:02,  4.52it/s]Loading checkpoint shards:  41%|████      | 7/17 [00:01<00:02,  4.53it/s]Loading checkpoint shards:  41%|████      | 7/17 [00:01<00:02,  4.52it/s]Loading checkpoint shards:  41%|████      | 7/17 [00:01<00:02,  4.52it/s]Loading checkpoint shards:  47%|████▋     | 8/17 [00:01<00:02,  4.45it/s]Loading checkpoint shards:  47%|████▋     | 8/17 [00:01<00:02,  4.45it/s]Loading checkpoint shards:  47%|████▋     | 8/17 [00:01<00:02,  4.45it/s]Loading checkpoint shards:  47%|████▋     | 8/17 [00:01<00:02,  4.45it/s]Loading checkpoint shards:  47%|████▋     | 8/17 [00:01<00:02,  4.44it/s]Loading checkpoint shards:  47%|████▋     | 8/17 [00:01<00:02,  4.44it/s]Loading checkpoint shards:  47%|████▋     | 8/17 [00:01<00:02,  4.44it/s]Loading checkpoint shards:  47%|████▋     | 8/17 [00:01<00:02,  4.45it/s]Loading checkpoint shards:  53%|█████▎    | 9/17 [00:01<00:01,  4.37it/s]Loading checkpoint shards:  53%|█████▎    | 9/17 [00:01<00:01,  4.37it/s]Loading checkpoint shards:  53%|█████▎    | 9/17 [00:01<00:01,  4.38it/s]Loading checkpoint shards:  53%|█████▎    | 9/17 [00:01<00:01,  4.37it/s]Loading checkpoint shards:  53%|█████▎    | 9/17 [00:01<00:01,  4.38it/s]Loading checkpoint shards:  53%|█████▎    | 9/17 [00:01<00:01,  4.38it/s]Loading checkpoint shards:  53%|█████▎    | 9/17 [00:01<00:01,  4.37it/s]Loading checkpoint shards:  53%|█████▎    | 9/17 [00:01<00:01,  4.37it/s]Loading checkpoint shards:  59%|█████▉    | 10/17 [00:02<00:01,  4.31it/s]Loading checkpoint shards:  59%|█████▉    | 10/17 [00:02<00:01,  4.31it/s]Loading checkpoint shards:  59%|█████▉    | 10/17 [00:02<00:01,  4.31it/s]Loading checkpoint shards:  59%|█████▉    | 10/17 [00:02<00:01,  4.31it/s]Loading checkpoint shards:  59%|█████▉    | 10/17 [00:02<00:01,  4.31it/s]Loading checkpoint shards:  59%|█████▉    | 10/17 [00:02<00:01,  4.31it/s]Loading checkpoint shards:  59%|█████▉    | 10/17 [00:02<00:01,  4.31it/s]Loading checkpoint shards:  59%|█████▉    | 10/17 [00:02<00:01,  4.30it/s]Loading checkpoint shards:  65%|██████▍   | 11/17 [00:02<00:01,  4.53it/s]Loading checkpoint shards:  65%|██████▍   | 11/17 [00:02<00:01,  4.53it/s]Loading checkpoint shards:  65%|██████▍   | 11/17 [00:02<00:01,  4.53it/s]Loading checkpoint shards:  65%|██████▍   | 11/17 [00:02<00:01,  4.53it/s]Loading checkpoint shards:  65%|██████▍   | 11/17 [00:02<00:01,  4.53it/s]Loading checkpoint shards:  65%|██████▍   | 11/17 [00:02<00:01,  4.53it/s]Loading checkpoint shards:  65%|██████▍   | 11/17 [00:02<00:01,  4.53it/s]Loading checkpoint shards:  65%|██████▍   | 11/17 [00:02<00:01,  4.53it/s]Loading checkpoint shards:  71%|███████   | 12/17 [00:02<00:01,  4.46it/s]Loading checkpoint shards:  71%|███████   | 12/17 [00:02<00:01,  4.46it/s]Loading checkpoint shards:  71%|███████   | 12/17 [00:02<00:01,  4.46it/s]Loading checkpoint shards:  71%|███████   | 12/17 [00:02<00:01,  4.46it/s]Loading checkpoint shards:  71%|███████   | 12/17 [00:02<00:01,  4.46it/s]Loading checkpoint shards:  71%|███████   | 12/17 [00:02<00:01,  4.46it/s]Loading checkpoint shards:  71%|███████   | 12/17 [00:02<00:01,  4.46it/s]Loading checkpoint shards:  71%|███████   | 12/17 [00:02<00:01,  4.46it/s]Loading checkpoint shards:  76%|███████▋  | 13/17 [00:02<00:00,  4.44it/s]Loading checkpoint shards:  76%|███████▋  | 13/17 [00:02<00:00,  4.44it/s]Loading checkpoint shards:  76%|███████▋  | 13/17 [00:02<00:00,  4.44it/s]Loading checkpoint shards:  76%|███████▋  | 13/17 [00:02<00:00,  4.44it/s]Loading checkpoint shards:  76%|███████▋  | 13/17 [00:02<00:00,  4.44it/s]Loading checkpoint shards:  76%|███████▋  | 13/17 [00:02<00:00,  4.44it/s]Loading checkpoint shards:  76%|███████▋  | 13/17 [00:02<00:00,  4.44it/s]Loading checkpoint shards:  76%|███████▋  | 13/17 [00:02<00:00,  4.44it/s]Loading checkpoint shards:  82%|████████▏ | 14/17 [00:03<00:00,  4.66it/s]Loading checkpoint shards:  82%|████████▏ | 14/17 [00:03<00:00,  4.66it/s]Loading checkpoint shards:  82%|████████▏ | 14/17 [00:03<00:00,  4.66it/s]Loading checkpoint shards:  82%|████████▏ | 14/17 [00:03<00:00,  4.66it/s]Loading checkpoint shards:  82%|████████▏ | 14/17 [00:03<00:00,  4.66it/s]Loading checkpoint shards:  82%|████████▏ | 14/17 [00:03<00:00,  4.66it/s]Loading checkpoint shards:  82%|████████▏ | 14/17 [00:03<00:00,  4.66it/s]Loading checkpoint shards:  82%|████████▏ | 14/17 [00:03<00:00,  4.66it/s]Loading checkpoint shards:  88%|████████▊ | 15/17 [00:03<00:00,  4.58it/s]Loading checkpoint shards:  88%|████████▊ | 15/17 [00:03<00:00,  4.58it/s]Loading checkpoint shards:  88%|████████▊ | 15/17 [00:03<00:00,  4.58it/s]Loading checkpoint shards:  88%|████████▊ | 15/17 [00:03<00:00,  4.58it/s]Loading checkpoint shards:  88%|████████▊ | 15/17 [00:03<00:00,  4.58it/s]Loading checkpoint shards:  88%|████████▊ | 15/17 [00:03<00:00,  4.58it/s]Loading checkpoint shards:  88%|████████▊ | 15/17 [00:03<00:00,  4.58it/s]Loading checkpoint shards:  88%|████████▊ | 15/17 [00:03<00:00,  4.58it/s]Loading checkpoint shards:  94%|█████████▍| 16/17 [00:03<00:00,  4.53it/s]Loading checkpoint shards:  94%|█████████▍| 16/17 [00:03<00:00,  4.53it/s]Loading checkpoint shards:  94%|█████████▍| 16/17 [00:03<00:00,  4.53it/s]Loading checkpoint shards:  94%|█████████▍| 16/17 [00:03<00:00,  4.53it/s]Loading checkpoint shards:  94%|█████████▍| 16/17 [00:03<00:00,  4.53it/s]Loading checkpoint shards:  94%|█████████▍| 16/17 [00:03<00:00,  4.53it/s]Loading checkpoint shards:  94%|█████████▍| 16/17 [00:03<00:00,  4.53it/s]Loading checkpoint shards:  94%|█████████▍| 16/17 [00:03<00:00,  4.53it/s]Loading checkpoint shards: 100%|██████████| 17/17 [00:03<00:00,  5.27it/s]Loading checkpoint shards: 100%|██████████| 17/17 [00:03<00:00,  5.27it/s]Loading checkpoint shards: 100%|██████████| 17/17 [00:03<00:00,  5.27it/s]Loading checkpoint shards: 100%|██████████| 17/17 [00:03<00:00,  5.27it/s]Loading checkpoint shards: 100%|██████████| 17/17 [00:03<00:00,  5.27it/s]Loading checkpoint shards: 100%|██████████| 17/17 [00:03<00:00,  5.26it/s]Loading checkpoint shards: 100%|██████████| 17/17 [00:03<00:00,  5.26it/s]Loading checkpoint shards: 100%|██████████| 17/17 [00:03<00:00,  4.74it/s]Loading checkpoint shards: 100%|██████████| 17/17 [00:03<00:00,  5.26it/s]
Loading checkpoint shards: 100%|██████████| 17/17 [00:03<00:00,  4.72it/s]Loading checkpoint shards: 100%|██████████| 17/17 [00:03<00:00,  4.72it/s]

Loading checkpoint shards: 100%|██████████| 17/17 [00:03<00:00,  4.72it/s]
Loading checkpoint shards: 100%|██████████| 17/17 [00:03<00:00,  4.73it/s]
Loading checkpoint shards: 100%|██████████| 17/17 [00:03<00:00,  4.72it/s]Loading checkpoint shards: 100%|██████████| 17/17 [00:03<00:00,  4.72it/s]
Loading checkpoint shards: 100%|██████████| 17/17 [00:03<00:00,  4.74it/s]

[rank0]: Traceback (most recent call last):
[rank0]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 1196, in <module>
[rank0]:     main()
[rank0]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 1179, in main
[rank0]:     evaluator = SafetyEvaluator(
[rank0]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 146, in __init__
[rank0]:     self._load_local_model(local_model_path or model)
[rank0]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 184, in _load_local_model
[rank0]:     self.local_model = self.accelerator.prepare(self.local_model)
[rank0]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1559, in prepare
[rank0]:     result = tuple(
[rank0]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1560, in <genexpr>
[rank0]:     self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
[rank0]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1402, in _prepare_one
[rank0]:     return self.prepare_model(obj, device_placement=device_placement)
[rank0]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1847, in prepare_model
[rank0]:     model = torch.nn.parallel.DistributedDataParallel(
[rank0]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 812, in __init__
[rank0]:     self._ddp_init_helper(
[rank0]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1152, in _ddp_init_helper
[rank0]:     self.reducer = dist.Reducer(
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 61.03 GiB. GPU 
[rank6]: Traceback (most recent call last):
[rank6]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 1196, in <module>
[rank6]:     main()
[rank6]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 1179, in main
[rank6]:     evaluator = SafetyEvaluator(
[rank6]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 146, in __init__
[rank6]:     self._load_local_model(local_model_path or model)
[rank6]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 184, in _load_local_model
[rank6]:     self.local_model = self.accelerator.prepare(self.local_model)
[rank6]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1559, in prepare
[rank6]:     result = tuple(
[rank6]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1560, in <genexpr>
[rank6]:     self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
[rank6]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1402, in _prepare_one
[rank6]:     return self.prepare_model(obj, device_placement=device_placement)
[rank6]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1847, in prepare_model
[rank6]:     model = torch.nn.parallel.DistributedDataParallel(
[rank6]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 812, in __init__
[rank6]:     self._ddp_init_helper(
[rank6]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1152, in _ddp_init_helper
[rank6]:     self.reducer = dist.Reducer(
[rank6]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 61.03 GiB. GPU  has a total capacity of 79.32 GiB of which 17.16 GiB is free. Including non-PyTorch memory, this process has 62.16 GiB memory in use. Of the allocated memory 61.03 GiB is allocated by PyTorch, and 1.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank5]: Traceback (most recent call last):
[rank5]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 1196, in <module>
[rank5]:     main()
[rank5]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 1179, in main
[rank5]:     evaluator = SafetyEvaluator(
[rank5]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 146, in __init__
[rank5]:     self._load_local_model(local_model_path or model)
[rank5]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 184, in _load_local_model
[rank5]:     self.local_model = self.accelerator.prepare(self.local_model)
[rank5]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1559, in prepare
[rank5]:     result = tuple(
[rank5]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1560, in <genexpr>
[rank5]:     self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
[rank5]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1402, in _prepare_one
[rank5]:     return self.prepare_model(obj, device_placement=device_placement)
[rank5]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1847, in prepare_model
[rank5]:     model = torch.nn.parallel.DistributedDataParallel(
[rank5]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 812, in __init__
[rank5]:     self._ddp_init_helper(
[rank5]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1152, in _ddp_init_helper
[rank5]:     self.reducer = dist.Reducer(
[rank5]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 61.03 GiB. GPU  has a total capacity of 79.32 GiB of which 17.16 GiB is free. Including non-PyTorch memory, this process has 62.16 GiB memory in use. Of the allocated memory 61.03 GiB is allocated by PyTorch, and 1.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank2]: Traceback (most recent call last):
[rank2]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 1196, in <module>
[rank2]:     main()
[rank2]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 1179, in main
[rank2]:     evaluator = SafetyEvaluator(
[rank2]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 146, in __init__
[rank2]:     self._load_local_model(local_model_path or model)
[rank2]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 184, in _load_local_model
[rank2]:     self.local_model = self.accelerator.prepare(self.local_model)
[rank2]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1559, in prepare
[rank2]:     result = tuple(
[rank2]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1560, in <genexpr>
[rank2]:     self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
[rank2]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1402, in _prepare_one
[rank2]:     return self.prepare_model(obj, device_placement=device_placement)
[rank2]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1847, in prepare_model
[rank2]:     model = torch.nn.parallel.DistributedDataParallel(
[rank2]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 812, in __init__
[rank2]:     self._ddp_init_helper(
[rank2]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1152, in _ddp_init_helper
[rank2]:     self.reducer = dist.Reducer(
[rank2]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 61.03 GiB. GPU  has a total capacity of 79.32 GiB of which 17.16 GiB is free. Including non-PyTorch memory, this process has 62.16 GiB memory in use. Of the allocated memory 61.03 GiB is allocated by PyTorch, and 1.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 1196, in <module>
[rank1]:     main()
[rank1]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 1179, in main
[rank1]:     evaluator = SafetyEvaluator(
[rank1]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 146, in __init__
[rank1]:     self._load_local_model(local_model_path or model)
[rank1]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 184, in _load_local_model
[rank1]:     self.local_model = self.accelerator.prepare(self.local_model)
[rank1]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1559, in prepare
[rank1]:     result = tuple(
[rank1]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1560, in <genexpr>
[rank1]:     self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
[rank1]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1402, in _prepare_one
[rank1]:     return self.prepare_model(obj, device_placement=device_placement)
[rank1]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1847, in prepare_model
[rank1]:     model = torch.nn.parallel.DistributedDataParallel(
[rank1]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 812, in __init__
[rank1]:     self._ddp_init_helper(
[rank1]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1152, in _ddp_init_helper
[rank1]:     self.reducer = dist.Reducer(
[rank1]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 61.03 GiB. GPU  has a total capacity of 79.32 GiB of which 17.16 GiB is free. Including non-PyTorch memory, this process has 62.16 GiB memory in use. Of the allocated memory 61.03 GiB is allocated by PyTorch, and 1.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank4]: Traceback (most recent call last):
[rank4]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 1196, in <module>
[rank4]:     main()
[rank4]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 1179, in main
[rank4]:     evaluator = SafetyEvaluator(
[rank4]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 146, in __init__
[rank4]:     self._load_local_model(local_model_path or model)
[rank4]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 184, in _load_local_model
[rank4]:     self.local_model = self.accelerator.prepare(self.local_model)
[rank4]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1559, in prepare
[rank4]:     result = tuple(
[rank4]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1560, in <genexpr>
[rank4]:     self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
[rank4]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1402, in _prepare_one
[rank4]:     return self.prepare_model(obj, device_placement=device_placement)
[rank4]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1847, in prepare_model
[rank4]:     model = torch.nn.parallel.DistributedDataParallel(
[rank4]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 812, in __init__
[rank4]:     self._ddp_init_helper(
[rank4]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1152, in _ddp_init_helper
[rank4]:     self.reducer = dist.Reducer(
[rank4]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 61.03 GiB. GPU  has a total capacity of 79.32 GiB of which 17.16 GiB is free. Including non-PyTorch memory, this process has 62.16 GiB memory in use. Of the allocated memory 61.03 GiB is allocated by PyTorch, and 1.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 1196, in <module>
[rank3]:     main()
[rank3]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 1179, in main
[rank3]:     evaluator = SafetyEvaluator(
[rank3]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 146, in __init__
[rank3]:     self._load_local_model(local_model_path or model)
[rank3]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 184, in _load_local_model
[rank3]:     self.local_model = self.accelerator.prepare(self.local_model)
[rank3]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1559, in prepare
[rank3]:     result = tuple(
[rank3]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1560, in <genexpr>
[rank3]:     self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
[rank3]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1402, in _prepare_one
[rank3]:     return self.prepare_model(obj, device_placement=device_placement)
[rank3]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1847, in prepare_model
[rank3]:     model = torch.nn.parallel.DistributedDataParallel(
[rank3]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 812, in __init__
[rank3]:     self._ddp_init_helper(
[rank3]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1152, in _ddp_init_helper
[rank3]:     self.reducer = dist.Reducer(
[rank3]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 61.03 GiB. GPU  has a total capacity of 79.32 GiB of which 17.16 GiB is free. Including non-PyTorch memory, this process has 62.16 GiB memory in use. Of the allocated memory 61.03 GiB is allocated by PyTorch, and 1.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank7]: Traceback (most recent call last):
[rank7]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 1196, in <module>
[rank7]:     main()
[rank7]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 1179, in main
[rank7]:     evaluator = SafetyEvaluator(
[rank7]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 146, in __init__
[rank7]:     self._load_local_model(local_model_path or model)
[rank7]:   File "/mnt/hwfile/liuhaoze/main/xlam_60k/evaluate_xlam_trustworthy_llm.py", line 184, in _load_local_model
[rank7]:     self.local_model = self.accelerator.prepare(self.local_model)
[rank7]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1559, in prepare
[rank7]:     result = tuple(
[rank7]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1560, in <genexpr>
[rank7]:     self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
[rank7]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1402, in _prepare_one
[rank7]:     return self.prepare_model(obj, device_placement=device_placement)
[rank7]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py", line 1847, in prepare_model
[rank7]:     model = torch.nn.parallel.DistributedDataParallel(
[rank7]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 812, in __init__
[rank7]:     self._ddp_init_helper(
[rank7]:   File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1152, in _ddp_init_helper
[rank7]:     self.reducer = dist.Reducer(
[rank7]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 61.03 GiB. GPU  has a total capacity of 79.32 GiB of which 17.25 GiB is free. Including non-PyTorch memory, this process has 62.06 GiB memory in use. Of the allocated memory 61.03 GiB is allocated by PyTorch, and 1.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W1202 17:18:30.501000 140599541389120 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 67297 closing signal SIGTERM
W1202 17:18:30.501000 140599541389120 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 67298 closing signal SIGTERM
W1202 17:18:30.501000 140599541389120 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 67299 closing signal SIGTERM
W1202 17:18:30.501000 140599541389120 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 67300 closing signal SIGTERM
W1202 17:18:30.501000 140599541389120 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 67301 closing signal SIGTERM
W1202 17:18:30.501000 140599541389120 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 67302 closing signal SIGTERM
W1202 17:18:30.501000 140599541389120 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 67303 closing signal SIGTERM
E1202 17:18:31.430000 140599541389120 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 0 (pid: 67296) of binary: /mnt/petrelfs/liuhaoze/anaconda3/bin/python
Traceback (most recent call last):
  File "/mnt/petrelfs/liuhaoze/anaconda3/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1272, in launch_command
    multi_gpu_launcher(args)
  File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/accelerate/commands/launch.py", line 899, in multi_gpu_launcher
    distrib_run.run(args)
  File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/torch/distributed/run.py", line 870, in run
    elastic_launch(
  File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/petrelfs/liuhaoze/anaconda3/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
evaluate_xlam_trustworthy_llm.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-12-02_17:18:30
  host      : HOST-10-140-66-127
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 67296)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
